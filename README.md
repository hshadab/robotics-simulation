# RoboSim - Interactive Robotics Simulation Platform

A web-based 3D robotics simulation platform built with React, Three.js, and Rapier physics. Designed for the **SO-101 robot arm** with AI-powered control, synthetic data generation, and direct HuggingFace integration.

## Features

### Robot Support
- **SO-101 Robot Arm** (Fully Supported) - 6-DOF open-source desktop arm from The Robot Studio
  - Realistic 3D model loaded from official URDF
  - STS3215 servo motors with 1/345 gear ratio
  - LeRobot (HuggingFace) Python export for real hardware
  - Full AI control, data recording, and policy execution
- **Wheeled Robot** (Coming Soon) - Differential drive mobile robot
- **Quadcopter Drone** (Coming Soon) - 4-motor drone with flight controls
- **Humanoid** (Coming Soon) - Bipedal robot with manipulation

### 3D Visualization
- Real-time 3D rendering with PBR materials
- Physics simulation using Rapier
- Multiple environment options (empty, warehouse, outdoor, maze)
- Contact shadows and studio lighting

### Interactive Controls (SO-101)
- Joint sliders for precise control
- Preset positions and animations
- Multiple control modes (manual, keyboard, gamepad, IK click-to-move)

### Advanced Arm Controls (SO-101)
- **Inverse Kinematics** - Click-to-move in 3D space with reachability preview
- **Numerical IK Solver** - Jacobian-based solver with damped least squares and CCD methods
- **Keyboard Teleoperation** - WASD + arrow keys for real-time control
- **Gamepad Support** - Full controller support with analog sticks
- **Task Templates** - Pre-programmed pick & place, stacking, and demo sequences
- **Trajectory Planning** - Smooth cubic/quintic interpolated motion paths
- **Workspace Visualization** - Semi-transparent dome showing reachable area

### Numerical Inverse Kinematics
- **Damped Least Squares (DLS)** - Singularity-robust Jacobian pseudo-inverse method
- **Cyclic Coordinate Descent (CCD)** - Fast iterative solver for chain kinematics
- **Multi-Start Optimization** - Try multiple initial configurations to find best solution
- **Trajectory Generation** - Smooth interpolated paths through IK waypoints
- **Manipulability Analysis** - Real-time measure of dexterity and singularity detection
- **Configurable Parameters** - Damping factor, step size, convergence tolerance
- **Joint Limit Handling** - Respects joint limits with configurable safety margins

### Real-time Monitoring
- **Joint Trajectory Graph** - Live plotting of all joint positions over time
- **Sensor Panel** - Joint angles, velocities, and gripper status

### Hardware Integration
- **Web Serial Connection** - Connect to real robot via USB (Chrome/Edge)
- **Auto-sync Mode** - Mirror simulation to hardware in real-time (30-60 Hz)
- **PWM Command Generation** - Automatic servo microsecond conversion

### Sensors & Visualization
- Robot camera view (picture-in-picture)
- Joint position/velocity sensors
- End-effector position tracking
- Gripper state monitoring

### Sensor Noise Models (Sim-to-Real Transfer)
- **Configurable Realism Levels** - None, Low, Medium, High, Extreme
- **Noise Types Supported**:
  - Gaussian noise (configurable standard deviation)
  - Systematic bias
  - Quantization (discrete sensor resolution)
  - Dropout (random sensor failures)
  - Lag/latency simulation
  - Jitter (timing variations)
  - Spike artifacts (sudden large errors)
- **Per-Sensor Profiles** - Realistic defaults for encoder, IMU, camera, and joint sensors
- **Real-time Toggle** - Enable/disable noise without restarting
- **UI Panel** - Intuitive controls in the Tools sidebar

### Robot Vision (Camera Simulation)
- **RGB Image Capture** - Capture frames from 3D viewport
- **Blob Detection** - Color-based object detection with connected components
- **Color Presets** - Red, Green, Blue, Yellow, Orange, Purple filters
- **HSV Filtering** - Industry-standard hue/saturation/value thresholds
- **Real-time Processing** - Configurable capture rate (1-30 FPS)
- **Visual Overlay** - Bounding boxes and centroids displayed on feed
- **Image Processing** - Edge detection, blur, brightness/contrast utilities

### State Persistence
- **Named Save Slots** - Up to 10 named saves with timestamps
- **Auto-Save** - Automatic background saving at configurable intervals
- **Import/Export** - Download and upload save files as JSON
- **IndexedDB Storage** - Large state data stored in browser database
- **Quick Resume** - Load auto-save to continue from last session
- **State Preview** - See robot type and timestamp before loading

### Multi-Robot Instances
- **Up to 8 Robots** - Run multiple SO-101 arm instances simultaneously
- **Formation Patterns** - Line, grid, circle, and V-formation layouts
- **Per-Robot State** - Each instance maintains independent joint states
- **Active Robot Selection** - Click to switch control focus between robots
- **Clone Robots** - Duplicate existing robots with offset positions
- **Collision Detection** - Automatic proximity checking between robots
- **Enable/Disable** - Toggle individual robot visibility and updates
- **Swarm Robotics Ready** - Foundation for multi-agent coordination

### AI Environment Generator (Gemini Integration)
- **AI-Generated Backgrounds** - Create custom scene backdrops using natural language
  - Presets: Warehouse, Garden, Laboratory, Space Station, Cartoon Workshop
  - Styles: Realistic, Cartoon, Abstract, Minimalist
  - Moods: Bright, Dark, Warm, Cool
- **AI-Generated Textures** - Create floor and wall textures with tiling support
  - Seamless textures for realistic surfaces
  - Materials: Concrete, Wood, Metal, Custom descriptions
- **AI-Generated Objects** - Create interactive objects the robot can manipulate
  - Shapes: Cube, Sphere, Cylinder
  - Styles: Realistic, Cartoon
  - Physics-enabled for picking and placing
- **Google Gemini API** - Powered by Gemini 2.0 Flash image generation
- **Scene Integration** - Apply generated content directly to 3D viewport
- **Download Option** - Save generated images for external use

### Voice Control (Web Speech API)
- **Hands-free Operation** - Control robots using voice commands
- **Wake Word Support** - Optional "Hey Robot" activation
- **Continuous Listening** - Keep listening for commands
- **Voice Feedback** - Spoken confirmations of actions
- **Command Categories**:
  - Movement: "move left", "go forward", "turn right"
  - Gripper: "open gripper", "grab object", "release"
  - Presets: "wave hello", "go home", "dance"
  - Queries: "where are you?", "what's your position?"
- **Browser Support** - Chrome, Edge (Web Speech API required)

### Vision-Language Analysis (Claude Vision)
- **Scene Understanding** - Ask "What's in the scene?" and get detailed answers
- **Object Detection** - Local DETR model for detecting objects
- **Scene Classification** - Identify environment types (warehouse, lab, outdoor)
- **Graspable Object Recognition** - Identify objects the robot can pick up
- **Spatial Queries** - "Where is the red object?", "What can I grab?"
- **Suggested Actions** - AI recommends next steps based on scene analysis
- **Dual Mode** - Works locally with Transformers.js, enhanced with Claude API

### Code Copilot (AI-Powered Editor)
- **Smart Autocomplete** - Robot API function suggestions
- **Code Generation** - Generate code from comments (Ctrl+Shift+G)
- **Code Explanation** - Explain selected code (Ctrl+Shift+E)
- **Error Fixing** - AI suggests fixes for runtime errors
- **Code Templates** - Pre-built patterns for common tasks
- **Works Offline** - Basic features work without API key

### Text-to-3D Model Generation
- **Natural Language Input** - Describe objects like "red apple" or "wooden box"
- **Procedural Meshes** - Generates appropriate 3D geometry
- **AI Textures** - Optional Gemini-powered texture generation
- **Multiple Styles** - Realistic, Cartoon, Low-poly, Voxel
- **Physics Enabled** - Generated objects work with robot interaction
- **Preset Objects** - Quick generation of common items
- **Scene Generation** - Create multiple objects from descriptions

### Image-to-3D Object Generation (ENHANCED)
- **Photo to 3D Model** - Upload real object photos and convert to training-ready 3D models
- **Multi-Image Support** - Upload up to 4 photos from different angles for better 3D reconstruction
- **Multiple API Services**:
  - **fal.ai (TripoSR)** - Fast (~10-30s), affordable ($0.07/gen)
  - **CSM.ai** - High quality, free tier (10 credits)
  - **Rodin (Hyper3D)** - Highest quality, multiple tiers
- **Auto Mesh Analysis** - Automatically extracts real dimensions from generated GLB
- **Smart Physics Colliders**:
  - Spherical objects â†’ Ball collider
  - Cylindrical objects â†’ Cylinder collider
  - Complex shapes â†’ Convex hull collider
  - Box-like objects â†’ Box collider
- **Improved Grasp Estimation** - Object-type-aware grasp points (bottles, tools, flat objects, etc.)
- **Volume-Based Mass** - Mass calculated from actual mesh volume
- **Download & Use** - Download GLB file or add directly to scene with physics
- **Training Pipeline** - Generates parameterized task templates for the object

### Code Editor
- Built-in JavaScript code editor with Monaco
- Robot API for programmatic control
- Code templates for common tasks
- Console output panel
- **AI Code Copilot** - Intelligent completions and suggestions

### Hardware Export
- **LeRobot Python** - Export to HuggingFace LeRobot framework for SO-101
- **Arduino** - Export to Arduino C++ for various hardware kits
- **MicroPython** - Export to MicroPython for ESP32/Raspberry Pi Pico

### AI Chat Assistant (Prompt-First Architecture)
- **Natural Language Control** - Describe what you want in plain English
- **Semantic State Awareness** - LLM sees robot state in natural language, not just raw numbers
- **Bidirectional Communication** - Robot events appear in chat (task completed, errors, etc.)
- **Context-Aware Responses** - LLM understands current pose, can do relative movements ("move a bit left")
- **Live Status Bar** - Real-time robot state display in chat panel
- **Clarifying Questions** - LLM can ask for more details when needed

### Policy Loading from HuggingFace Hub
- **Browse LeRobot Policies** - Search and discover trained policies from HuggingFace Hub
- **ONNX Runtime** - Run policies locally in browser using ONNX Runtime Web
- **SafeTensors Support** - Load SafeTensors model weights directly (native HuggingFace format)
- **Policy Types Supported** - ACT (Action Chunking Transformer), Diffusion, TD-MPC, VQ-BeT
- **SO-101 Compatible** - Filter and load policies trained for SO-101/Koch robot arms
- **Real-time Inference** - Execute policies at 20Hz for smooth robot control
- **No Server Required** - All inference runs client-side in WebAssembly

### LeRobot Dataset Recording & Export (ENHANCED)
- **LeRobot v3.0 Compatible** - Full compatibility with HuggingFace LeRobot format
- **Complete 6-Joint Capture** - Records all SO-101 joints (base, shoulder, elbow, wrist, wristRoll, gripper)
- **Accurate Statistics** - Proper standard deviation calculation for feature normalization
- **Multi-Camera Recording** - Support for cam_high, cam_wrist, cam_left, cam_right views
- **Dataset Statistics Dashboard** - Analyze episode counts, success rates, joint coverage
- **Quality Recommendations** - Get feedback on data quality for training readiness
- **Dataset Browser** - Browse and preview LeRobot datasets from HuggingFace Hub
- **HuggingFace Upload** - Direct upload with proper LeRobot directory structure
- **Task Success Detection** - Automatic detection for reach, pick & place, push, stack tasks
- **Python Conversion Script** - Included `convert_to_parquet.py` for true Parquet format
- **Auto-Generated README** - Dataset documentation included in exports

### Interactive Tutorial System
- **Guided Tutorials** - Step-by-step walkthroughs for new users
- **Three Modules** - Getting Started, AI Features, Data Collection
- **Progress Tracking** - Completion status saved locally
- **Panel Highlighting** - Points to relevant UI elements
- **Tips & Hints** - Contextual tips for each step

### Guided Challenge System (NEW)
- **Interactive Challenges** - Learn robot control through hands-on exercises
- **Real-time Position Validation** - Visual feedback as you match target positions
- **Three Challenge Levels**:
  - Basic Movement: Learn individual joint controls
  - Reach Position: Practice moving to specific poses
  - Pick Motion Sequence: Master pick-and-place workflow
- **Progress Tracking** - Step-by-step indicators with auto-advance
- **Hints & Success Messages** - Get help when stuck, celebrate completions
- **Tolerance-based Matching** - Configurable accuracy requirements

### Parameterized Task Templates (NEW)
- **Configurable Waypoints** - Define robot motions with variable parameters
- **Variable References** - Use `${paramName}` syntax for dynamic values
- **Built-in Templates**:
  - Pick-and-Place: Configurable pick/place positions
  - Stack Objects: Multi-height stacking sequences
  - Reach Target: Precision positioning tasks
  - Wave Hello: Demo animation with timing control
- **Parameter Randomization** - Auto-generate variations for training data
- **Custom Template Creation** - Define your own parameterized tasks

### Visual Domain Randomization (NEW)
- **Lighting Controls** - Ambient intensity, directional light, shadow softness
- **Material Variations** - Metalness, roughness, color tinting
- **Camera Randomization** - FOV, position jitter for viewpoint diversity
- **Preset Configurations**:
  - Bright Studio: Clean, well-lit environment
  - Moody: Dark, dramatic lighting
  - Outdoor: Natural sunlight simulation
  - Factory: Industrial lighting conditions
- **Auto-Randomize Mode** - Continuously vary visual parameters
- **Sim-to-Real Ready** - Prepare policies for real-world transfer

### Dataset Augmentation (NEW)
- **Trajectory Augmentation** - Multiply datasets with variations
- **Augmentation Types**:
  - Action Noise: Gaussian noise on joint targets
  - Time Stretching: Speed up/slow down trajectories
  - Spatial Jitter: Small position variations
  - Temporal Dropout: Skip frames for robustness
  - Mirror/Flip: Create symmetric variations
- **Preview Before Apply** - See augmentation effects before generating
- **Configurable Multiplier** - 2x to 10x dataset expansion
- **Quality Preservation** - Maintains trajectory smoothness

### Auto-Episode Generator
- **One-Click Generation** - Create 100+ episodes instantly
- **Template-Based** - Uses parameterized task templates
- **Randomized Parameters** - Each episode has unique variations
- **Combined with Augmentation** - Base episodes Ã— augmentation multiplier
- **Progress Tracking** - Real-time generation progress
- **Export Options**:
  - LeRobot Format: Direct HuggingFace compatibility
  - JSON Export: For custom training pipelines
- **Estimated Output** - Preview episode count before generating

### Object Library (NEW)
- **34 Physics Objects** - Cubes, balls, cylinders with realistic physics
- **YCB Benchmark Objects** - Standard robotics research objects (soup cans, boxes, tools)
- **6 Object Categories** - Containers, Food, Tools, Toys, Kitchen, Office
- **7 Scene Presets** - Pre-configured manipulation scenarios:
  - Block Stacking, Multi-Block Stack, Cup Pouring
  - Color Sorting, Fruit Pick & Place, Can Lineup, Office Desk
- **One-Click Setup** - Load complete scenes instantly
- **Add/Remove Objects** - Build custom manipulation environments

### LLM â†’ Physics Recording (NEW)
- **Natural Language to Data** - Type "Stack the red block on blue" â†’ generates training episodes
- **Physics Simulation** - Runs actual simulation with Rapier physics engine
- **Camera Capture** - Records RGB frames at 30 FPS during execution
- **Language-Conditioned Datasets** - Instructions embedded in metadata for RT-1/OpenVLA
- **Batch Generation** - Generate 1-50 varied episodes per instruction
- **Scene Integration** - Uses Object Library presets for realistic scenes
- **Motion Plan Parsing** - AI converts instructions to robot waypoints
- **Export Ready** - Direct LeRobot v3.0 format with images + language

### Chat â†’ Training Data (NEW)
- **Live Recording from Chat** - Every chat command becomes a labeled training episode
- **Auto-Record Mode** - Automatically captures demonstrations as you chat
- **Session Management** - Start/stop recording sessions with full control
- **Quality Metrics** - Real-time smoothness, velocity, and duration tracking
- **Success/Fail Labeling** - Mark episodes for filtering during training
- **Language Instructions** - Chat messages become language labels automatically
- **Natural Demonstrations** - Create diverse data through natural conversation

### Quick Train Flow (NEW - Apple-Inspired UX)
- **One-Button Wizard** - Minimalist step-by-step flow: Add Object â†’ Record Demo â†’ Generate â†’ Upload
- **Standard Object Library** - 34 physics-enabled objects (cubes, balls, cylinders) ready to use instantly
- **Photo to 3D** - Upload a photo and convert to training-ready 3D model via fal.ai
- **Chat-Based Recording** - Say "pick up the block" and the demo is auto-recorded
- **Auto-Stop Recording** - Recording stops automatically when robot finishes moving
- **Suggested Prompts** - Contextual command suggestions based on your object
- **Direct HuggingFace Upload** - One-click export with automatic Parquet conversion
- **Tools Drawer** - All advanced tools hidden in slide-out panel for minimal distraction

### Guided Teleoperation Recording (NEW)
- **Task Templates** - Pre-defined tasks: Pick & Place, Stacking, Pushing, Waypoint Navigation
- **Step-by-Step Visual Guides** - 3D overlays show target positions, arrows, and ghost gripper
- **Real-Time Quality Indicators** - Smoothness score, velocity tracking, duration monitoring
- **Automatic Language Generation** - Task templates include varied language instructions
- **Keyboard/Gamepad Teleoperation** - WASD + gamepad support for smoother demonstrations
  - Keyboard: WASD (base/shoulder), QE (elbow), RF (wrist), ZXC (gripper/roll)
  - Gamepad: Left stick (base/shoulder), Right stick (elbow/wrist), Triggers (gripper)
- **Quality Scoring** - Episodes rated as excellent/good/acceptable/poor
- **Dataset Statistics Dashboard** - Comprehensive analysis before export

### Enhanced Teleoperation for Dataset Collection
- **Smooth Acceleration** - Cubic easing for natural robot motion
- **Configurable Speed** - Shift to speed up, Ctrl to slow down
- **Preset Positions** - H for home, G for ready position
- **Dead Zone Control** - Adjustable gamepad sensitivity
- **Visual Feedback** - Active joint highlighting during control

### HuggingFace Hub Integration (NEW)
- **Direct Upload** - Push datasets to HuggingFace without CLI
- **Token Authentication** - Secure API token validation
- **Auto Repository Creation** - Creates dataset repos automatically
- **Dataset Card Generation** - Auto-generates README with metadata
- **Public/Private Toggle** - Control dataset visibility
- **Upload Progress** - Real-time progress tracking
- **Direct Link** - Open uploaded dataset in browser

### Minimal UI Design (Apple-Inspired)
- **One-Button Flow** - Main interface shows only the Quick Train wizard
- **Progressive Disclosure** - Complex features hidden until needed
- **Slide-Out Tools Drawer** - Access all 20+ panels via settings button
- **Step-Based Wizard** - Clear progression: Add Object â†’ Demo â†’ Generate â†’ Upload
- **Distraction-Free** - Focus on the task, not the UI

### Physics Simulation Realism (NEW)

RoboSim implements several features to ensure training data transfers well to real robots:

#### Inverse Kinematics-Based Motion
- **IK Pick-up Sequences** - Commands like "pick up the cube" use real inverse kinematics instead of hardcoded heuristics
- **Approach/Grasp/Lift Planning** - Each pick-up calculates three IK solutions (approach, grasp, lift) for smooth trajectories
- **Fallback Mode** - If IK fails (unreachable position), gracefully falls back to heuristic control
- **Single FK Source** - All gripper position calculations use the official SO-101 URDF parameters

#### Realistic Motor Dynamics
- **Velocity-Limited Motion** - Joints respect the STS3215 servo's 180Â°/s maximum velocity
- **Rise Time Simulation** - 150ms first-order response matching real servo behavior
- **S-Curve Easing** - Quintic S-curve (not cubic) for smooth acceleration/deceleration
- **Minimum Duration Enforcement** - Large movements automatically take longer (velocity-limited)

#### Why This Matters for Sim-to-Real
| Feature | Before | After | Real Robot |
|---------|--------|-------|------------|
| Joint velocity | Instant | 180Â°/s max | 180Â°/s max âœ“ |
| Motion profile | Cubic ease | S-curve | S-curve âœ“ |
| Pick-up planning | Heuristic | IK-based | IK-based âœ“ |
| FK accuracy | Hardcoded | URDF-derived | URDF-derived âœ“ |

Training data generated with these improvements will transfer better to real SO-101 hardware because the simulated trajectories match what the real servos can actually achieve.

## Tech Stack

- **Frontend**: React 18, TypeScript
- **3D Graphics**: Three.js, React Three Fiber, React Three Drei
- **Physics**: Rapier (via @react-three/rapier)
- **ML Inference**: ONNX Runtime Web, HuggingFace Transformers.js
- **State Management**: Zustand
- **Styling**: Tailwind CSS
- **Build Tool**: Vite

## Getting Started

### Prerequisites
- Node.js 18+
- npm or yarn

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/robotics-simulation.git
cd robotics-simulation

# Install dependencies
npm install

# Start development server
npm run dev
```

The application will be available at `http://localhost:5173`

### Build for Production

```bash
npm run build
```

## Technical Approach: Accurate Robot Models

This section documents the methodology used to create physically accurate robot simulations from real robot specifications.

### Step 1: Source Official Robot Data

For accurate simulations, start with official manufacturer data:

- **URDF files** - Unified Robot Description Format from the robot's official repository
- **STL meshes** - 3D geometry files linked in the URDF
- **Joint limits** - From URDF `<limit>` tags (converted from radians to degrees)
- **Gear ratios** - From manufacturer documentation (e.g., LeRobot docs for SO-101)

For SO-101, we use:
```
public/models/so101/
â”œâ”€â”€ so101.urdf           # Official URDF from TheRobotStudio/SO-ARM100
â””â”€â”€ meshes/
    â”œâ”€â”€ base_link.stl
    â”œâ”€â”€ shoulder_link.stl
    â”œâ”€â”€ upper_arm_link.stl
    â”œâ”€â”€ forearm_link.stl
    â”œâ”€â”€ wrist_link.stl
    â”œâ”€â”€ gripper_link.stl
    â””â”€â”€ sts3215_*.stl    # Servo motor meshes
```

### Step 2: URDF Parsing with urdf-loader

Use the `urdf-loader` library to parse URDF and load STL meshes:

```typescript
import URDFLoader from 'urdf-loader';

const loader = new URDFLoader();
loader.packages = '/models/so101';  // Base path for mesh loading

// Custom STL loader with materials
loader.loadMeshCb = (path, manager, onComplete) => {
  const stlLoader = new STLLoader(manager);
  stlLoader.load(path, (geometry) => {
    const isServo = path.includes('sts3215');
    const material = isServo ? SERVO_MATERIAL : PRINTED_MATERIAL;
    const mesh = new THREE.Mesh(geometry, material);
    onComplete(mesh);
  });
};

loader.load('/models/so101/so101.urdf', (robot) => {
  // Robot is now a Three.js object with articulated joints
  robot.rotation.x = -Math.PI / 2;  // Z-up (URDF) to Y-up (Three.js)
});
```

### Step 3: Apply Realistic Materials

Differentiate between 3D-printed parts and servo motors:

```typescript
// 3D printed plastic (PLA/PETG)
const PRINTED_MATERIAL = new THREE.MeshStandardMaterial({
  color: '#F5F0E6',  // Off-white filament
  metalness: 0.0,
  roughness: 0.4,
});

// STS3215 servo motors
const SERVO_MATERIAL = new THREE.MeshStandardMaterial({
  color: '#1a1a1a',  // Black plastic housing
  metalness: 0.2,
  roughness: 0.3,
});
```

### Step 4: Joint Control via URDF

The urdf-loader provides `setJointValue()` for each joint:

```typescript
// Map UI names to URDF joint names
const JOINT_MAP = {
  base: 'shoulder_pan',
  shoulder: 'shoulder_lift',
  elbow: 'elbow_flex',
  wrist: 'wrist_flex',
  wristRoll: 'wrist_roll',
  gripper: 'gripper',
};

// Update joints (convert degrees to radians)
robot.joints[JOINT_MAP.shoulder].setJointValue((angle * Math.PI) / 180);
```

### Step 5: Self-Collision Prevention

Since URDF models are purely kinematic (no inter-link collision), we implement software constraints to prevent impossible poses:

```typescript
// src/lib/selfCollision.ts
export function preventSelfCollision(joints: JointState, robotId: string): JointState {
  if (robotId !== 'so-101') return joints;

  const corrected = { ...joints };

  // Shoulder+Elbow constraint: prevent arm folding through base
  // More shoulder tilt = less elbow can fold back
  if (corrected.shoulder > 40) {
    const minSum = -10;
    if (corrected.shoulder + corrected.elbow < minSum) {
      corrected.elbow = minSum - corrected.shoulder;
    }
  }

  return corrected;
}
```

This is applied in the Zustand store whenever joints are updated.

### Step 6: Physics Integration

Wrap the URDF model in Rapier physics:

```typescript
<RigidBody type="fixed" colliders={false}>
  <CuboidCollider args={[0.06, 0.04, 0.06]} position={[0, 0.04, 0]} />
  <primitive object={robot} />
</RigidBody>
```

The base is fixed, and the arm moves kinematically (driven by joint angles rather than physics forces).

### Benefits of This Approach

1. **Accuracy** - Real dimensions, joint limits, and gear ratios from manufacturer
2. **Visual fidelity** - Actual 3D geometry, not approximations
3. **Hardware compatibility** - Same joint names/limits as real hardware
4. **Maintainability** - Update by replacing URDF/STL files from upstream

## Architecture: Prompt-First Robot Control

RoboSim is designed with a **prompt-first** architecture where natural language is the primary interface for robot control, not an afterthought.

### How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  USER PROMPT                                            â”‚
â”‚  "Pick up the block and move it to the left"            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SEMANTIC STATE (Natural Language Context)              â”‚
â”‚                                                         â”‚
â”‚  "Arm is rotated 45Â° left, gripper open.                â”‚
â”‚   End effector at (0.15m, 0.12m, 0.08m).                â”‚
â”‚   Last action: wave completed 10s ago."                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM (Claude/GPT)                                       â”‚
â”‚  - Understands spatial relationships                    â”‚
â”‚  - Can reference current position ("from here")         â”‚
â”‚  - Generates trajectory or asks clarifying questions    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ROBOT EXECUTION                                        â”‚
â”‚  - Smooth trajectory interpolation                      â”‚
â”‚  - Task tracking (start/complete/fail)                  â”‚
â”‚  - Event emission for feedback                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CHAT FEEDBACK                                          â”‚
â”‚  "ğŸ¤– Task completed: Picked up object"                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

| Component | File | Purpose |
|-----------|------|---------|
| Robot Context | `src/lib/robotContext.ts` | Central state + event bus |
| Semantic State | `src/lib/semanticState.ts` | Converts state to natural language |
| Claude API | `src/lib/claudeApi.ts` | LLM integration with semantic context |
| Chat Panel | `src/components/chat/ChatPanel.tsx` | Bidirectional UI |

### Example Conversation

```
User: "Wave hello"
Assistant: "Waving hello!"
ğŸ¤– Task completed: Waving hello!

User: "Now move a bit to the left"
Assistant: "I see you're currently at 45Â° base rotation.
            Moving left to 75Â°."
ğŸ¤– Task completed: Moving left

User: "What's the gripper doing?"
Assistant: "The gripper is currently fully open, positioned
            at medium height, reaching forward."
```

## Project Structure

```
src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ simulation/      # 3D robot components
â”‚   â”‚   â”œâ”€â”€ SO101Arm3D.tsx       # SO-101 arm with URDF
â”‚   â”‚   â”œâ”€â”€ SO101Kinematics.ts   # Forward/Inverse kinematics
â”‚   â”‚   â”œâ”€â”€ ClickToMove.tsx      # IK-based click targeting
â”‚   â”‚   â”œâ”€â”€ WheeledRobot3D.tsx
â”‚   â”‚   â”œâ”€â”€ Drone3D.tsx
â”‚   â”‚   â”œâ”€â”€ Humanoid3D.tsx
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ controls/        # UI control panels
â”‚   â”‚   â”œâ”€â”€ ConsolidatedToolsPanel.tsx # Tabbed tool categories (Control, AI, Data, Hardware, Settings)
â”‚   â”‚   â”œâ”€â”€ AdvancedControlsPanel.tsx  # IK, keyboard, gamepad modes
â”‚   â”‚   â”œâ”€â”€ TaskTemplatesPanel.tsx     # Pick & place sequences
â”‚   â”‚   â”œâ”€â”€ PolicyBrowserPanel.tsx     # HuggingFace policy loader
â”‚   â”‚   â”œâ”€â”€ DatasetBrowserPanel.tsx    # Browse LeRobot datasets
â”‚   â”‚   â”œâ”€â”€ DatasetStatsPanel.tsx      # Dataset quality analysis
â”‚   â”‚   â”œâ”€â”€ TutorialPanel.tsx          # Interactive tutorials
â”‚   â”‚   â”œâ”€â”€ AIEnvironmentPanel.tsx     # AI-generated environments and objects
â”‚   â”‚   â”œâ”€â”€ VoiceControlPanel.tsx      # Voice command interface
â”‚   â”‚   â”œâ”€â”€ VisionAnalysisPanel.tsx    # Scene understanding with AI
â”‚   â”‚   â”œâ”€â”€ TextTo3DPanel.tsx          # Text-to-3D object generation
â”‚   â”‚   â”œâ”€â”€ ImageTo3DPanel.tsx         # Image-to-3D with CSM API
â”‚   â”‚   â”œâ”€â”€ ObjectLibraryPanel.tsx     # Physics object library browser
â”‚   â”‚   â”œâ”€â”€ LLMRecordingPanel.tsx      # LLM â†’ Physics recording
â”‚   â”‚   â”œâ”€â”€ JointTrajectoryGraph.tsx   # Real-time plotting
â”‚   â”‚   â”œâ”€â”€ SerialConnectionPanel.tsx  # Hardware connection
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ editor/          # Code editor components
â”‚   â”œâ”€â”€ chat/            # AI chat interface
â”‚   â””â”€â”€ layout/          # Layout components
â”œâ”€â”€ hooks/               # Custom React hooks
â”‚   â”œâ”€â”€ useTrajectoryExecution.ts  # Smooth motion execution
â”‚   â”œâ”€â”€ useRobotContext.ts         # Robot state + events hook
â”‚   â”œâ”€â”€ useCodeCopilot.ts          # AI code completion for Monaco
â”‚   â””â”€â”€ ...
â”œâ”€â”€ lib/                 # Robot APIs and utilities
â”‚   â”œâ”€â”€ robotContext.ts        # Central state aggregator + event bus
â”‚   â”œâ”€â”€ semanticState.ts       # Natural language state translation
â”‚   â”œâ”€â”€ sensorNoise.ts         # Realistic sensor noise models
â”‚   â”œâ”€â”€ visionSimulation.ts    # Camera capture and blob detection
â”‚   â”œâ”€â”€ statePersistence.ts    # Save/load state with IndexedDB
â”‚   â”œâ”€â”€ multiRobot.ts          # Multi-robot instance management
â”‚   â”œâ”€â”€ numericalIK.ts         # Jacobian-based inverse kinematics solver
â”‚   â”œâ”€â”€ aiImageGeneration.ts   # Gemini AI image generation for environments
â”‚   â”œâ”€â”€ voiceControl.ts        # Web Speech API voice commands
â”‚   â”œâ”€â”€ visionLanguage.ts      # Vision-language scene analysis
â”‚   â”œâ”€â”€ codeCopilot.ts         # AI code completion and generation
â”‚   â”œâ”€â”€ textTo3D.ts            # Text-to-3D model generation
â”‚   â”œâ”€â”€ csmImageTo3D.ts        # CSM API for image-to-3D conversion
â”‚   â”œâ”€â”€ objectTaskGenerator.ts # Auto-generate task templates for objects
â”‚   â”œâ”€â”€ objectLibrary.ts       # Physics object definitions (YCB + primitives)
â”‚   â”œâ”€â”€ physicsEpisodeGenerator.ts # LLM â†’ Physics episode recording
â”‚   â”œâ”€â”€ logger.ts              # Structured logging utility
â”‚   â”œâ”€â”€ huggingfaceHub.ts      # HuggingFace Hub API integration
â”‚   â”œâ”€â”€ policyRunner.ts        # ONNX Runtime policy execution
â”‚   â”œâ”€â”€ safetensorsLoader.ts   # SafeTensors model format loader
â”‚   â”œâ”€â”€ parquetWriter.ts       # Apache Parquet file writer
â”‚   â”œâ”€â”€ taskDetection.ts       # Task success detection
â”‚   â”œâ”€â”€ huggingfaceUpload.ts   # HuggingFace Hub dataset upload
â”‚   â”œâ”€â”€ trajectoryPlanner.ts   # Motion interpolation
â”‚   â”œâ”€â”€ serialConnection.ts    # Web Serial API
â”‚   â””â”€â”€ ...
â”œâ”€â”€ stores/              # Zustand state management
â”œâ”€â”€ config/              # Robot profiles, environments
â””â”€â”€ types/               # TypeScript type definitions
```

## Robot APIs

### SO-101 Robot Arm
```javascript
// Joint control (6-DOF)
robot.moveJoint('base', 45);      // Rotate base (shoulder_pan)
robot.moveJoint('shoulder', 30);  // Lift shoulder (shoulder_lift)
robot.moveJoint('elbow', -60);    // Bend elbow (elbow_flex)
robot.moveJoint('wrist', 20);     // Flex wrist (wrist_flex)
robot.moveJoint('wristRoll', 90); // Roll wrist (wrist_roll)

// Gripper control
robot.openGripper();              // Open gripper
robot.closeGripper();             // Close gripper
robot.setGripper(50);             // Set gripper to 50%

// Preset positions
robot.goHome();                   // Return to home position
```

### Keyboard Controls (SO-101)

Enable keyboard mode in the Advanced Controls panel:

| Key | Action |
|-----|--------|
| W/S | Shoulder up/down |
| A/D | Base rotate left/right |
| â†‘/â†“ | Elbow up/down |
| â†/â†’ | Wrist up/down |
| Q/E | Wrist roll left/right |
| Space | Open gripper |
| Shift | Close gripper |

### Gamepad Controls (SO-101)

| Control | Action |
|---------|--------|
| Left Stick X | Base rotation |
| Left Stick Y | Shoulder angle |
| Right Stick X | Wrist angle |
| Right Stick Y | Elbow angle |
| Left Bumper/Right Bumper | Wrist roll |
| Left Trigger | Close gripper |
| Right Trigger | Open gripper |

### Wheeled Robot (Coming Soon)
```javascript
robot.forward(150);               // Drive forward
robot.backward(100);              // Drive backward
robot.turnLeft(100);              // Turn left
robot.turnRight(100);             // Turn right
robot.stop();                     // Stop motors
```

### Drone (Coming Soon)
```javascript
drone.arm();                      // Arm motors
drone.takeoff(0.5);               // Take off to height (meters)
drone.land();                     // Land the drone
drone.setThrottle(60);            // Set throttle (0-100)
```

## Hardware Connection

### Web Serial (Real-time Mirror)

Connect directly to your SO-101 from the browser (Chrome/Edge required):

1. Click "Connect" in the Hardware Connection panel
2. Select your USB serial port (usually `/dev/ttyUSB0` or `COM3`)
3. Enable "Auto-sync" to mirror simulation to hardware in real-time

The default protocol sends servo PWM commands at configurable rates (1-60 Hz):
```
J0:1500,J1:1500,J2:1500,J3:1500,J4:1500,J5:1500
```

Configure your Arduino/ESP32 to parse this format and drive servos accordingly.

### Supported Baud Rates
- 9600, 19200, 38400, 57600, 115200 (default), 250000, 500000, 1000000

## Hardware Export

### LeRobot Python (SO-101)

Export your simulation code to run on real SO-101 hardware using the HuggingFace LeRobot framework:

```python
from lerobot.common.robot_devices.motors.feetech import FeetechMotorsBus

# Generated code includes SO101Controller class
robot = SO101Controller(port="/dev/ttyUSB0")
robot.move_joint("shoulder", 45)
robot.go_home()
robot.disconnect()
```

Setup for real hardware:
```bash
pip install lerobot
pip install -e ".[feetech]"  # For STS3215 servo support
lerobot-find-port             # Discover serial port
lerobot-calibrate             # Calibrate arm positions
```

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

MIT License - see LICENSE file for details.

## Acknowledgments

- [The Robot Studio](https://www.therobotstudio.com/) - SO-101 robot arm design and URDF
- [HuggingFace LeRobot](https://github.com/huggingface/lerobot) - Robot learning framework
- [React Three Fiber](https://github.com/pmndrs/react-three-fiber)
- [Rapier Physics](https://rapier.rs/)
- [Zustand](https://github.com/pmndrs/zustand)
- Berkeley Humanoid Lite design inspiration

## Development Roadmap

RoboSim aims to be the fastest path from zero to trained robot policy. The following features are planned to address key pain points in robotics education and data collection.

### Phase 1: Onboarding Improvements (Quick Wins)
- [x] **First-run tutorial modal** - Detect new users and prompt interactive onboarding
- [x] **Batch episode recording** - "Record N episodes" button for efficient data collection
- [x] **Trajectory noise augmentation** - Add configurable noise to recorded episodes for diversity

### Phase 2: Data Generation Tools (Medium Effort)
- [x] **Parameterized task templates** - Configurable waypoints with randomizable parameters
- [x] **Visual randomization UI** - Lighting, texture, and color variation controls
- [x] **Dataset augmentation panel** - Multiply episodes with automated variations

### Phase 3: Advanced Features (Major)
- [x] **Auto-episode generator** - One-click synthetic data generation (100+ episodes)
- [x] **Guided challenge system** - Interactive tutorials with position validation
- [x] **Direct HuggingFace upload** - Integrated Hub publishing without CLI

### Why These Features?

Based on research into robotics simulation pain points:

1. **Learning Curve**: Traditional tools like ROS/Gazebo require complex installation and version matching. RoboSim runs in any browser with zero setup.

2. **Data Collection Cost**: Imitation learning requires 50-200+ demonstration episodes, taking days of manual teleoperation. Automated generation can reduce this to minutes.

3. **Sim-to-Real Transfer**: Domain randomization (visual, sensor, trajectory) is essential for policies that work on real hardware.

See [ROADMAP.md](./ROADMAP.md) for detailed implementation plans.

## Resources

- [SO-101 Official Repository](https://github.com/TheRobotStudio/SO-ARM100)
- [LeRobot SO-101 Documentation](https://huggingface.co/docs/lerobot/so101)
- [SO-101 Assembly Tutorial](https://maegantucker.com/ECE4560/assignment6-so101/)
